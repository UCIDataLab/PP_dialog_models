{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove pkl files to re-generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# for fname in os.listdir(\"./\"):\n",
    "#     if fname.endswith(\"pkl\"):\n",
    "#         os.remove('./'+fname)\n",
    "#         print \"Removing \"+fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data\n",
    "## File Paths\n",
    "Set the data paths (for training, only when it's available)<br>\n",
    "Download sample data here: [train](https://drive.google.com/open?id=1HN5L6kkh9mYa7vQ_W-H9InW_kgQbFfrR), [test](https://drive.google.com/open?id=1s_P7IrmGJFN6OTLKOrQSUz8TTcyNF6fS) (Only accessible to PCORI team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_data_file = './mhd.4.25.18_sample_tr.txt'\n",
    "te_data_file = './mhd.4.25.18_sample_te.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classes\n",
    "Training and test data classes are slightly different since labels and vocabulary are determined only at the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mhddata import MHDTrainData, MHDTestData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data: Below lines will run inside the function `.fit_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing the corpus with labels\n",
      "  Cleaning the corpus (removing punctuations..)\n",
      "            0 utterances\n",
      "         5000 utterances\n",
      "        10000 utterances\n",
      "        15000 utterances\n",
      "        20000 utterances\n",
      "        25000 utterances\n",
      "        30000 utterances\n",
      "        35000 utterances\n",
      "        40000 utterances\n",
      "        45000 utterances\n",
      "        50000 utterances\n",
      "        55000 utterances\n",
      "        60000 utterances\n",
      "        65000 utterances\n",
      "        70000 utterances\n",
      "        75000 utterances\n",
      "        80000 utterances\n",
      "        85000 utterances\n",
      "        90000 utterances\n",
      "  Extracting noun phrases..\n",
      "Cleaning labels ..\n",
      "  16 OtherAddictions --> 37 Other\n",
      "  18 Death --> 37 Other\n",
      "  19 Bereavement --> 37 Other\n",
      "  20 PainSuffering --> 37 Other\n",
      "  24 ActivityDailyLiving --> 37 Other\n",
      "  26 Unemployment --> 37 Other\n",
      "  27 MoneyBenefits --> 37 Other\n",
      "  28 Caregiver --> 37 Other\n",
      "  31 Religion --> 37 Other\n",
      "  32 Age --> 37 Other\n",
      "  33 LivingWillAdvanceCarePlanning --> 37 Other\n",
      "  35 MDPT-Relationship --> 37 Other\n",
      "  5 Prognosis --> 37 Other\n",
      "Getting lists of valid session/utterance IDs that have both text and labels\n"
     ]
    }
   ],
   "source": [
    "mhdtrain = MHDTrainData(tr_data_file, nouns_only=False, ignore_case=True,\n",
    "                 remove_numbers=False, sub_numbers=True, stopwords_dir=\"./stopwordlists\",\n",
    "                 label_mappings=None, ngram_range=(1,1), max_np_len=2, min_wlen=1,\n",
    "                 min_dfreq=0, max_dfreq=0.8, min_sfreq=20,\n",
    "                 token_pattern=r\"(?u)[A-Za-z\\?\\!\\-\\.']+\", verbose=3,  # can control verbosity\n",
    "                 corpus_pkl='./corpus.pkl', label_pkl='./label.pkl', vocab_pkl='./vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions: 209 (ones that have text)\n",
      "Number of sessions: 209 (ones that have labels)\n",
      "Number of sessions: 209 (ones that have both text and labels)\n",
      "Number of segments: 6565 (ones that have both text and labels)\n",
      "Number of utterances: 92739 (ones that have both text and labels)\n",
      "Number of labels that originally had: 38 (including the ones that appear in the sessions without text)\n",
      "Number of labels: 25 (after cleaning the labels)\n",
      "Vocabulary size: 13091\n",
      "Number of user-defined stopwords: 553\n",
      "Number of stopwords used in total: 553 (including the words with low dfs and high dfs)\n"
     ]
    }
   ],
   "source": [
    "mhdtrain.print_stats()  # you could always print out the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(list(mhdtrain.vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data: Below lines will run inside the function `.predict ` or `.predict_viterbi` (for HMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels file from ./label.pkl\n",
      " (Delete the file if you want to re-generate the labels)\n",
      "Loading cleaned labels file from ./label_cleaned.pkl\n",
      "Loading the vocabulary file from ./vocab.pkl\n",
      " (Delete the file if you want to re-generate the vocabulary)\n",
      "Loading and preprocessing the corpus with labels\n",
      "Loading the processed file from ./corpus_te.pkl\n",
      " (Delete the file if you want to re-process the corpus)\n",
      "Cleaning labels ..\n",
      "  16 OtherAddictions --> 37 Other\n",
      "  18 Death --> 37 Other\n",
      "  19 Bereavement --> 37 Other\n",
      "  20 PainSuffering --> 37 Other\n",
      "  24 ActivityDailyLiving --> 37 Other\n",
      "  26 Unemployment --> 37 Other\n",
      "  27 MoneyBenefits --> 37 Other\n",
      "  28 Caregiver --> 37 Other\n",
      "  31 Religion --> 37 Other\n",
      "  32 Age --> 37 Other\n",
      "  33 LivingWillAdvanceCarePlanning --> 37 Other\n",
      "  35 MDPT-Relationship --> 37 Other\n",
      "  5 Prognosis --> 37 Other\n"
     ]
    }
   ],
   "source": [
    "mhdtest = MHDTestData(te_data_file, nouns_only=False, ignore_case=True,\n",
    "                 remove_numbers=False, sub_numbers=True, stopwords_dir=\"./stopwordlists\",\n",
    "                 label_mappings=None, ngram_range=(1,1), max_np_len=2, min_wlen=1,\n",
    "                 min_dfreq=0, max_dfreq=0.8, min_sfreq=20,\n",
    "                 token_pattern=r\"(?u)[A-Za-z\\?\\!\\-\\.']+\", verbose=3,\n",
    "                 corpus_pkl='./corpus_te.pkl', tr_label_pkl='./label.pkl', tr_vocab_pkl='./vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression Models\n",
    "## 1.1 Train & Predict\n",
    "### Train (This step will not needed if you're not training and loading the pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihyun/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Logistic regression model to ./lrdialog_ovr.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import LogRegDialogModel\n",
    "\n",
    "lr = LogRegDialogModel(lr_type='ovr')\n",
    "lr.fit_model(tr_data_file, penalty_type=\"l2\", reg_const=1.0,\n",
    "             model_file='./lrdialog_ovr.pkl', verbose=0)  # Saves model to model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "Prediction creates `lr.result`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels file from ./label.pkl\n",
      " (Delete the file if you want to re-generate the labels)\n",
      "Loading cleaned labels file from ./label_cleaned.pkl\n",
      "Loading the vocabulary file from ./vocab.pkl\n",
      " (Delete the file if you want to re-generate the vocabulary)\n",
      "Loading and preprocessing the corpus with labels\n",
      "Loading the processed file from ./corpus_te.pkl\n",
      " (Delete the file if you want to re-process the corpus)\n",
      "Cleaning labels ..\n",
      "Calculating scores..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihyun/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/jihyun/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.DialogResult instance at 0x132536d88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(te_data_file, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 34.82142857142857,\n",
       " 'auc': 0.5435482799766377,\n",
       " 'auc_w': 0.5739195353237183,\n",
       " 'f1score': 0.14652481268845458,\n",
       " 'f1score_w': 0.29283037656735433,\n",
       " 'precision': 0.38311016246092167,\n",
       " 'precision_w': 0.4180309387651325,\n",
       " 'recall': 0.12134763011459579,\n",
       " 'recall_w': 0.3791459075340366,\n",
       " 'rprecision': 0.10704220148834033,\n",
       " 'rprecision_w': 0.20824991390702363}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.result.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can print and save it to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model,accuracy,precision_w,recall_w,auc_w,rprecision_w,f1score_w,precision,recall,auc,rprecision,f1score\n",
      "LogReg_ovr_l2_0.9 ,34.7354 ,0.4206 ,0.3784 ,0.5727 ,0.2063 ,0.2910 ,0.3826 ,0.1195 ,0.5426 ,0.1052 ,0.1440\n"
     ]
    }
   ],
   "source": [
    "lr.result.print_scores(filename='./result_in_diff_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load & Predict\n",
    "`lr2` loads the model that was trained above (part that we're going to release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Logistic regression model to ./lrdialog_ovr.pkl\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogRegDialogModel(lr_type='ovr')\n",
    "lr2.load_model(model_file=\"./lrdialog_ovr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating scores..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.DialogResult instance at 0x131635b00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.predict(te_data_file, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 34.735449735449734,\n",
       " 'auc': 0.5425712732040822,\n",
       " 'auc_w': 0.572677043648532,\n",
       " 'f1score': 0.14403660616037472,\n",
       " 'f1score_w': 0.2910227733934047,\n",
       " 'precision': 0.38264151805597885,\n",
       " 'precision_w': 0.42059459080558814,\n",
       " 'recall': 0.11948464842998098,\n",
       " 'recall_w': 0.3783846298101865,\n",
       " 'rprecision': 0.10519838445328382,\n",
       " 'rprecision_w': 0.20626242019434446}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.result.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below code is just to test out if the HMM on top of any base class runs fine by loading predictions and out probs.\n",
    "import cPickle as cp\n",
    "with open('./sample_prob.pkl', 'wb') as f:\n",
    "    cp.dump(lr2.result.output_prob, f)\n",
    "with open('./sample_pred.pkl', 'wb') as f:\n",
    "    cp.dump(lr2.result.predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Run GridSearch CV)\n",
    "You could do a cross-validation to find the best parameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] C=0.5 ...........................................................\n",
      "[CV] ............................................ C=0.5, total=   8.6s\n",
      "[CV] C=0.5 ...........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................................ C=0.5, total=   8.8s\n",
      "[CV] C=0.5 ...........................................................\n",
      "[CV] ............................................ C=0.5, total=   9.9s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=  12.1s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=   9.9s\n",
      "[CV] C=1.0 ...........................................................\n",
      "[CV] ............................................ C=1.0, total=  10.6s\n",
      "[CV] C=1.5 ...........................................................\n",
      "[CV] ............................................ C=1.5, total=  11.8s\n",
      "[CV] C=1.5 ...........................................................\n",
      "[CV] ............................................ C=1.5, total=  11.4s\n",
      "[CV] C=1.5 ...........................................................\n",
      "[CV] ............................................ C=1.5, total=  12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization constant: 1.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1.5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lr.grid_search_parameter(tr_data_file, C_values=np.arange(0.5, 2, 0.5), n_fold=3, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HMM on top of LR\n",
    "Running HMM requires you to have `base_model`, which should be trained in advance and given as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing the corpus with labels\n",
      "Loading the processed file from ./corpus.pkl\n",
      " (Delete the file if you want to re-process the corpus)\n",
      "Loading the vocabulary file from ./vocab.pkl\n",
      " (Delete the file if you want to re-generate the vocabulary)\n",
      "Loading labels file from ./label.pkl\n",
      " (Delete the file if you want to re-generate the labels)\n",
      "Cleaning labels ..\n",
      "Getting lists of valid session/utterance IDs that have both text and labels\n",
      "Saving model to hmmdialog_lrovr.pkl\n"
     ]
    }
   ],
   "source": [
    "from models import HMMDialogModel\n",
    "\n",
    "hmmlr = HMMDialogModel(base_model=lr)\n",
    "hmmlr.fit_model(tr_data_file, model_file='hmmdialog_lrovr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels file from ./label.pkl\n",
      " (Delete the file if you want to re-generate the labels)\n",
      "Loading cleaned labels file from ./label_cleaned.pkl\n",
      "Loading the vocabulary file from ./vocab.pkl\n",
      " (Delete the file if you want to re-generate the vocabulary)\n",
      "Loading and preprocessing the corpus with labels\n",
      "Loading the processed file from ./corpus_te.pkl\n",
      " (Delete the file if you want to re-process the corpus)\n",
      "Cleaning labels ..\n",
      "Calculating scores..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.DialogResult instance at 0x1439fb9e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmmlr.predict_viterbi(te_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 61.07804232804233,\n",
       " 'auc': 0.7450265747286064,\n",
       " 'auc_w': 0.7891203038559265,\n",
       " 'f1score': 0.4746712926281416,\n",
       " 'f1score_w': 0.6121240696688772,\n",
       " 'precision': 0.4890888276638108,\n",
       " 'precision_w': 0.6294373210326876,\n",
       " 'recall': 0.5070740452021653,\n",
       " 'recall_w': 0.6164666637512746,\n",
       " 'rprecision': 0.4191670434480562,\n",
       " 'rprecision_w': 0.5650249909207642}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmmlr.result.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HMM on top of other output probabilities\n",
    "\n",
    "If we have a set of results from another base model (independent model) that is trained somewhere else (e.g. output from RNN), <br>\n",
    "we can load the predictions and output probabilities and plug them into HMM. <br>\n",
    "They should be the result of the same data as `mhdtest`.\n",
    "- `predictions`:  Should have a list of sessions, where each session is a 2-d array with size `(N,T)`, where `N` is the number of utterances in the session and `T` is the number of topics (labels). Each entry is the $p(topic|utterance)$ in each session.  <br> Type: `list[ 2-d np.array[float] ]`.\n",
    "- `output_probs`: Should have a list of sessions, where each session is a list of utterance predictions within that session. <br> Type: `list[list[int]]` or `list[np.array[int]]`\n",
    "\n",
    "\n",
    "After loading predictions and probabilities, a base model object should have the following data\n",
    "and it can be plugged in as an argument to HMMDialogModel\n",
    "- base_model.result\n",
    "- base_model.result.output_prob\n",
    "- base_model.model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import DialogModel, HMMDialogModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predfile = './sample_pred.pkl'\n",
    "outprobfile = './sample_prob.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not from RNN, but let's say we've loaded the results from RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels file from ./label.pkl\n",
      " (Delete the file if you want to re-generate the labels)\n",
      "Loading cleaned labels file from ./label_cleaned.pkl\n",
      "Loading the vocabulary file from ./vocab.pkl\n",
      " (Delete the file if you want to re-generate the vocabulary)\n",
      "Loading and preprocessing the corpus with labels\n",
      "Loading the processed file from ./corpus_te.pkl\n",
      " (Delete the file if you want to re-process the corpus)\n",
      "Cleaning labels ..\n",
      "Calculating scores..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.DialogResult instance at 0x12876a710>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = DialogModel()\n",
    "rnn.load_results(te_data_file, model_info=\"RNN\", marginals=None, predictions=predfile, output_probs=outprobfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing the corpus with labels\n",
      "Loading the processed file from ./corpus.pkl\n",
      " (Delete the file if you want to re-process the corpus)\n",
      "Loading the vocabulary file from ./vocab.pkl\n",
      " (Delete the file if you want to re-generate the vocabulary)\n",
      "Loading labels file from ./label.pkl\n",
      " (Delete the file if you want to re-generate the labels)\n",
      "Cleaning labels ..\n",
      "Getting lists of valid session/utterance IDs that have both text and labels\n",
      "Saving model to ./hmmdialog.pkl\n"
     ]
    }
   ],
   "source": [
    "hmmrnn = HMMDialogModel(base_model=rnn)\n",
    "hmmrnn.fit_model(tr_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels file from ./label.pkl\n",
      " (Delete the file if you want to re-generate the labels)\n",
      "Loading cleaned labels file from ./label_cleaned.pkl\n",
      "Loading the vocabulary file from ./vocab.pkl\n",
      " (Delete the file if you want to re-generate the vocabulary)\n",
      "Loading and preprocessing the corpus with labels\n",
      "Loading the processed file from ./corpus_te.pkl\n",
      " (Delete the file if you want to re-process the corpus)\n",
      "Cleaning labels ..\n",
      "Calculating scores..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.DialogResult instance at 0x1662f5ea8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmmrnn.predict_viterbi(te_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we should have the same result as the result at section 2. since we've loaded the same result from LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 60.826719576719576,\n",
       " 'auc': 0.7386513157025536,\n",
       " 'auc_w': 0.7880021417563148,\n",
       " 'f1score': 0.4611548266387942,\n",
       " 'f1score_w': 0.6094283413948923,\n",
       " 'precision': 0.47253030001967306,\n",
       " 'precision_w': 0.6258756621745318,\n",
       " 'recall': 0.4944229819272277,\n",
       " 'recall_w': 0.614199668112505,\n",
       " 'rprecision': 0.4151477738181185,\n",
       " 'rprecision_w': 0.5668641644422592}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmmlr.result.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
