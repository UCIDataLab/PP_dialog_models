{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: This notebook is not going to be released to public\n",
    "A modified version will be released to public. This is the internal version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove pkl files to re-generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# for fname in os.listdir(\"./\"):\n",
    "#     if fname.endswith(\"pkl\"):\n",
    "#         os.remove('./'+fname)\n",
    "#         print \"Removing \"+fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data\n",
    "## File Paths\n",
    "Set the data paths (for training, only when it's available)<br>\n",
    "Download sample data here: [train](https://drive.google.com/open?id=1HN5L6kkh9mYa7vQ_W-H9InW_kgQbFfrR), [test](https://drive.google.com/open?id=1s_P7IrmGJFN6OTLKOrQSUz8TTcyNF6fS) (Only accessible to PCORI team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_data_file = './mhd.4.25.18_sample_tr.txt'\n",
    "te_data_file = './mhd.4.25.18_sample_te.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classes\n",
    "Data preprocessing is done at the initialization step when creating data classes.<br>\n",
    "Training and test data classes are slightly different since labels and vocabulary are determined only at the training step. <br> \n",
    "An object of class `MHDTrainData` should be put in as an argument for `.fit_model` function, <br>\n",
    "and an object of class `MHDTestData` should be plugged into the `.predict_*` function for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mhddata import MHDTrainData, MHDTestData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mhdtrain = MHDTrainData(tr_data_file, nouns_only=False, ignore_case=True,\n",
    "                 remove_numbers=False, sub_numbers=True, stopwords_dir=\"./stopwordlists\",\n",
    "                 label_mappings=None, ngram_range=(1,1), max_np_len=2, min_wlen=1,\n",
    "                 min_dfreq=0, max_dfreq=0.8, min_sfreq=20,\n",
    "                 token_pattern=r\"(?u)[A-Za-z\\?\\!\\-\\.']+\", verbose=3,  # can control verbosity\n",
    "                 corpus_pkl='./corpus.pkl', label_pkl='./label.pkl', vocab_pkl='./vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mhdtrain.print_stats()  # you could always print out the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(list(mhdtrain.vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mhdtest = MHDTestData(te_data_file, nouns_only=False, ignore_case=True,\n",
    "                 remove_numbers=False, sub_numbers=True, proper_nouns_dir=\"./stopwordlists\",\n",
    "                 min_wlen=1, token_pattern=r\"(?u)[A-Za-z\\?\\!\\-\\.']+\", verbose=3, reload_corpus=True,\n",
    "                 corpus_pkl='./corpus_te.pkl', tr_label_pkl='./label.pkl', tr_vocab_pkl='./vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression Models\n",
    "Create an object of class LogRegDialogModel.<br>\n",
    "`lr_type` can be either `ovr` for one vs. rest model, or `multinomial` for multinomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import LogRegDialogModel\n",
    "lr = LogRegDialogModel(lr_type='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Train & Predict\n",
    "### Train (This step will not needed if you're loading the pre-trained model)\n",
    "1) Trains a LR model using training data. `lr.model` is created.<br>\n",
    "2) saves the model into pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr.fit_model(mhdtrain, penalty_type=\"l2\", reg_const=1.0,\n",
    "             model_file='./lrdialog_ovr.pkl', verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "1) Plug in the test data for prediction. `lr.predict()` uses `lr.model` and predict on the test data. <br>\n",
    "2) Prediction creates `lr.result` object. Also outputs an utterance-level results to file `output_filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.predict(mhdtest, verbose=1, output_filename='./utter_level_results_lrovr.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.result.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can print and save it to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.result.print_scores(filename='./result_in_diff_metrics_lrovr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load & Predict\n",
    "`lr2` loads the model that was trained above (part that we're going to release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr2 = LogRegDialogModel(lr_type='ovr')\n",
    "lr2.load_model(model_file=\"./lrdialog_ovr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr2.predict(mhdtest, verbose=1, output_filename='./utter_level_results_lrovr2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results should be the same as above since we used the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr2.result.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the output probability and predictions to pkl files.\n",
    "Below code is just to test out if the HMM on top of any base class runs fine by loading predictions and out probs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as cp\n",
    "with open('./sample_prob.pkl', 'wb') as f:\n",
    "    cp.dump(lr2.result.output_prob, f)\n",
    "with open('./sample_pred.pkl', 'wb') as f:\n",
    "    cp.dump(lr2.result.predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Run GridSearch CV to find the best parameter `C`\n",
    "You could do a cross-validation to find the best parameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lr.grid_search_parameter(mhdtrain, C_values=np.arange(0.5, 2, 0.5),\n",
    "                          penalty_type=\"l2\", solver='lbfgs',\n",
    "                          n_fold=3, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HMM on top of LR\n",
    "Running HMM requires you to have an object of **`base_model`**, which should be trained and predicted in advance and given as an argument. <br>\n",
    "The object has to have `.result` field since HMM is using the output probabilities from the model. \n",
    "<br>Here we use the logistic regression model that was trained and predicted above.<br>\n",
    "**NOTE: The base model and the HMM should be trained with the same data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from models import HMMDialogModel\n",
    "hmmlr = HMMDialogModel(base_model=lr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM pickle file has transition probabilities as well as start and ending probabilities.<br>\n",
    "You could also load the pre-trained model if available. (Commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hmmlr.fit_model(mhdtrain, model_file='hmmdialog.pkl', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hmmlr.load_model(model_file='hmmdialog.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmmlr.predict_viterbi(mhdtest, output_filename='./utter_level_result_hmmlrovr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmmlr.result.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HMM on top of other output probabilities\n",
    "\n",
    "If we have a set of results from another base model (independent model) that is trained somewhere else (e.g. output from RNN), <br>\n",
    "we can load the predictions and output probabilities and plug them into HMM. <br>\n",
    "They should be the result of the same data as `mhdtest`.\n",
    "- `predictions`:  Should have a list of sessions, where each session is a 2-d array with size `(N,T)`, where `N` is the number of utterances in the session and `T` is the number of topics (labels). Each entry is the $p(topic|utterance)$ in each session.  <br> Type: `list[ 2-d np.array[float] ]`.\n",
    "- `output_probs`: Should have a list of sessions, where each session is a list of utterance predictions within that session. <br> Type: `list[list[int]]` or `list[np.array[int]]`\n",
    "\n",
    "\n",
    "After loading predictions and probabilities, a base model object should have the following data\n",
    "and it can be plugged in as an argument to HMMDialogModel\n",
    "- base_model.result\n",
    "- base_model.result.output_prob\n",
    "- base_model.model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import DialogModel, HMMDialogModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the pkl files that we saved above\n",
    "predfile = './sample_pred.pkl'\n",
    "outprobfile = './sample_prob.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not from RNN, but let's say we've loaded the results from RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnn = DialogModel()\n",
    "rnn.load_results(mhdtest, model_info=\"RNN\", marginals=None, predictions=predfile, output_probs=outprobfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmmrnn = HMMDialogModel(base_model=rnn)\n",
    "hmmrnn.load_model(model_file='hmmdialog.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hmmrnn.predict_viterbi(mhdtest, output_filename='./utter_level_result_fake_rnn.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we should have the same result as the result at section 2. since we've loaded the same result from LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmmlr.result.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
